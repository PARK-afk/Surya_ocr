import cv2
from PIL import Image, ImageEnhance, ImageFilter, ImageOps
import numpy as np
import re
import os
import time
from surya.recognition import RecognitionPredictor
from surya.detection import DetectionPredictor
from surya.layout import LayoutPredictor
from paddleocr import PaddleOCR


class OCRProcessor:
	"""ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ í´ë˜ìŠ¤"""
	
	def __init__(self, max_height=2000, overlap_ratio=0.1, debug=False):
		"""
		Args:
			max_height: ì´ë¯¸ì§€ ë¶„í•  ê¸°ì¤€ ìµœëŒ€ ë†’ì´
			overlap_ratio: ë¶„í•  êµ¬ê°„ ê°„ ì¤‘ì²© ë¹„ìœ¨
			debug: ë””ë²„ê·¸ ëª¨ë“œ
		"""
		self.max_height = max_height
		self.overlap_ratio = overlap_ratio
		self.debug = debug
		self.recognition_predictor = None
		self.detection_predictor = None
		self.paddle_ocr = None
	
	def _init_predictors(self):
		"""Surya ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
		if self.recognition_predictor is None:
			from surya.foundation import FoundationPredictor
			foundation_predictor = FoundationPredictor()
			self.recognition_predictor = RecognitionPredictor(foundation_predictor)
			self.detection_predictor = DetectionPredictor()
	
	def _init_paddle_ocr(self, use_gpu=False):
		"""PaddleOCR ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
		if self.paddle_ocr is None:
			self.paddle_ocr = PaddleOCR(use_textline_orientation=True, lang='korean')
	
	def smart_image_split(self, image_path):
		"""
		ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• 
		
		Returns:
			ë¶„í• ëœ ì´ë¯¸ì§€ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì™€ ê°ê°ì˜ Y ì˜¤í”„ì…‹ ì •ë³´
		"""
		img = Image.open(image_path)
		width, height = img.size
		
		if height <= self.max_height:
			if self.debug: 
				print(f"ğŸ“ ë¶„í•  ë¶ˆí•„ìš”: ë†’ì´ {height} <= {self.max_height}")
			return [(img, 0)]
		
		if self.debug: 
			print(f"ğŸ“ ê¸´ ì´ë¯¸ì§€ ê°ì§€: {width}x{height} -> ë¶„í•  ì‹œì‘")
		
		# 1ë‹¨ê³„: ë¹ ë¥¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ìœ¼ë¡œ ì•ˆì „í•œ ë¶„í• ì  ì°¾ê¸°
		safe_split_points = self._find_safe_split_points(img)
		
		# 2ë‹¨ê³„: ì•ˆì „í•œ ë¶„í• ì ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„í• 
		image_segments = []
		overlap_height = int(self.max_height * self.overlap_ratio)
		
		for i, (start_y, end_y) in enumerate(safe_split_points):
			# ì¤‘ì²© ì˜ì—­ ì¶”ê°€ (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ì œì™¸)
			actual_start = max(0, start_y - (overlap_height if i > 0 else 0))
			actual_end = min(height, end_y + (overlap_height if i < len(safe_split_points) - 1 else 0))
			
			# ì´ë¯¸ì§€ ë¶„í• 
			segment = img.crop((0, actual_start, width, actual_end))
			image_segments.append((segment, start_y))  # ì›ë³¸ì—ì„œì˜ Y ì˜¤í”„ì…‹ ì €ì¥
			
			if self.debug: 
				print(f"ğŸ“¸ ë¶„í•  {i+1}: Y {actual_start}-{actual_end} (ì›ë³¸ ê¸°ì¤€: {start_y}-{end_y})")
		
		return image_segments

	def _find_safe_split_points(self, img):
		"""í…ìŠ¤íŠ¸ ì˜ì—­ì„ í”¼í•´ì„œ ì•ˆì „í•œ ë¶„í• ì  ì°¾ê¸°"""
		width, height = img.size
		
		# ê°„ë‹¨í•œ ë°©ë²•: ê°€ë¡œ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°€ë„ ë¶„ì„
		gray_img = img.convert('L')
		img_array = np.array(gray_img)
		
		# ê° í–‰ì˜ í”½ì…€ ë³€í™”ëŸ‰ ê³„ì‚° (í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë³€í™”ëŸ‰ì´ í¼)
		horizontal_projection = []
		for y in range(height):
			row = img_array[y, :]
			# í”½ì…€ê°’ ë³€í™”ëŸ‰ ê³„ì‚°
			diff = np.sum(np.abs(np.diff(row.astype(int))))
			horizontal_projection.append(diff)
		
		# ë³€í™”ëŸ‰ì„ ì •ê·œí™”
		max_diff = max(horizontal_projection) if horizontal_projection else 1
		normalized_projection = [x / max_diff for x in horizontal_projection]
		
		if self.debug:
			print(f"ğŸ“Š ê°€ë¡œ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ì™„ë£Œ")
		
		# ë¶„í• ì  í›„ë³´ ì°¾ê¸°
		split_candidates = []
		current_y = 0
		
		while current_y < height:
			target_end = min(current_y + self.max_height, height)
			
			if target_end >= height:
				# ë§ˆì§€ë§‰ êµ¬ê°„
				split_candidates.append((current_y, height))
				break
			
			# ëª©í‘œ ì§€ì  ì£¼ë³€ì—ì„œ ê°€ì¥ ì•ˆì „í•œ ë¶„í• ì  ì°¾ê¸°
			search_start = max(current_y + self.max_height - 200, current_y + self.max_height // 2)
			search_end = min(target_end + 200, height)
			
			best_split_y = self._find_best_split_in_range(
				normalized_projection, search_start, search_end
			)
			
			split_candidates.append((current_y, best_split_y))
			current_y = best_split_y
		
		if self.debug:
			print(f"ğŸ“ ë¶„í• ì  í›„ë³´: {len(split_candidates)}ê°œ")
			for i, (start, end) in enumerate(split_candidates):
				print(f"  êµ¬ê°„ {i+1}: {start} - {end} (ë†’ì´: {end-start})")
		
		return split_candidates

	def _find_best_split_in_range(self, projection, start_y, end_y):
		"""ì£¼ì–´ì§„ ë²”ìœ„ì—ì„œ ê°€ì¥ ì•ˆì „í•œ ë¶„í• ì  ì°¾ê¸°"""
		if start_y >= end_y or start_y >= len(projection):
			return end_y
		
		# ê²€ìƒ‰ ë²”ìœ„ ì œí•œ
		actual_start = max(0, start_y)
		actual_end = min(len(projection), end_y)
		
		# ë²”ìœ„ ë‚´ì—ì„œ í…ìŠ¤íŠ¸ ë°€ë„ê°€ ê°€ì¥ ë‚®ì€ ì§€ì  ì°¾ê¸°
		min_density = float('inf')
		best_y = actual_end
		
		# ì—°ì†ëœ ì—¬ëŸ¬ í–‰ì˜ í‰ê·  ë°€ë„ë¡œ íŒë‹¨ (ë” ì•ˆì •ì )
		window_size = 5
		
		for y in range(actual_start, actual_end - window_size):
			# ìœˆë„ìš° í‰ê·  ê³„ì‚°
			window_avg = sum(projection[y:y+window_size]) / window_size
			
			if window_avg < min_density:
				min_density = window_avg
				best_y = y + window_size // 2
		
		# í…ìŠ¤íŠ¸ ë°€ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ê°•ì œë¡œ ëì  ì‚¬ìš©
		if min_density > 0.3:  # ì„ê³„ê°’ ì¡°ì • ê°€ëŠ¥
			best_y = actual_end
			if self.debug: 
				print(f"âš ï¸ ì•ˆì „í•œ ë¶„í• ì  ì—†ìŒ, ê°•ì œ ë¶„í•  at {best_y}")
		
		return best_y

	def enhanced_preprocess(self, image_path):
		"""í•œêµ­ì–´ OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ê³ ê¸‰ ì „ì²˜ë¦¬"""
		# PILë¡œ ì´ë¯¸ì§€ ì½ê¸°
		img = Image.open(image_path)
		original_size = img.size
		
		# 1. ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë” ì •í™•í•œ ê¸°ì¤€)
		width, height = img.size
		target_height = 2400  # í•œêµ­ì–´ OCRì— ìµœì í™”ëœ ë†’ì´ (ë†’ì¼ìˆ˜ë¡ ì •í™•ë„ ì¦ê°€)
		if height < target_height:
			scale = target_height / height
			new_width = int(width * scale)
			new_height = int(height * scale)
			img = img.resize((new_width, new_height), Image.LANCZOS)
			if self.debug: 
				print(f"ğŸ“ í¬ê¸° ì¡°ì •: {original_size} -> {img.size}")
		
		# 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
		if img.mode != 'L':
			img = img.convert('L')
		
		# 3. íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” (ì¡°ëª… ë¶ˆê· í˜• í•´ê²°)
		img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_GRAY2BGR)
		img_cv_gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
		clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8,8))  # ëŒ€ë¹„ ê°•í™”
		img_clahe = clahe.apply(img_cv_gray)
		img = Image.fromarray(img_clahe)
		if self.debug: 
			print("ğŸ“Š íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” ì ìš©")
		
		# 4. ì ì‘ì  ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (ì´ë¯¸ì§€ í’ˆì§ˆì— ë”°ë¼ ì¡°ì •)
		blur_radius = self._calculate_optimal_blur(img)
		if blur_radius > 0:
			img = img.filter(ImageFilter.GaussianBlur(radius=blur_radius))
			if self.debug: 
				print(f"ğŸŒ€ ë¸”ëŸ¬ ì ìš©: radius={blur_radius}")
		
		# 5. ëŒ€ë¹„ ê°•í™”
		enhancer = ImageEnhance.Contrast(img)
		img = enhancer.enhance(1.8)  # ëŒ€ë¹„ ì¦ê°€ (1.2 -> 1.8)
		
		# 6. RGBë¡œ ë³€í™˜ (Surya í˜¸í™˜ì„±)
		img = img.convert('RGB')
		
		if self.debug: 
			print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
		return img

	def _calculate_optimal_blur(self, img):
		"""ì´ë¯¸ì§€ í’ˆì§ˆì— ë”°ë¥¸ ìµœì  ë¸”ëŸ¬ ë°˜ì§€ë¦„ ê³„ì‚°"""
		img_array = np.array(img)
		
		# ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°ìœ¼ë¡œ ì´ë¯¸ì§€ ì„ ëª…ë„ ì¸¡ì •
		laplacian_var = cv2.Laplacian(img_array, cv2.CV_64F).var()
		
		if laplacian_var > 500:  # ë§¤ìš° ì„ ëª…
			return 0.3
		elif laplacian_var > 200:  # ë³´í†µ
			return 0.5
		elif laplacian_var > 100:  # ì•½ê°„ íë¦¼
			return 0.7
		else:  # ë§¤ìš° íë¦¼
			return 0  # ë¸”ëŸ¬ ì ìš© ì•ˆí•¨

	def _get_confidence_icon(self, confidence):
		"""ì‹ ë¢°ë„ì— ë”°ë¥¸ ì‹œê°ì  ì•„ì´ì½˜ ë°˜í™˜"""
		if confidence >= 0.9:
			return "ğŸŸ¢"  # ë†’ì€ ì‹ ë¢°ë„ (ì´ˆë¡)
		elif confidence >= 0.7:
			return "ğŸŸ¡"  # ë³´í†µ ì‹ ë¢°ë„ (ë…¸ë‘)
		else:
			return "ğŸ”´"  # ë‚®ì€ ì‹ ë¢°ë„ (ë¹¨ê°•)

	def fix_korean_ocr_errors(self, text):
		"""í•œêµ­ì–´ OCR í”í•œ ì˜¤ì¸ì‹ íŒ¨í„´ ìˆ˜ì •"""
		corrections = {
			r'(\d)\s+(\d)': r'\1\2',          # ìˆ«ì ì‚¬ì´ ê³µë°± ì œê±°
			r'(\d+,?\d*)ä¹³': r'\g<1>ì›',       # ìˆ«ì ë’¤ ä¹³ -> ì›
			r'2,000ä¹³': '2,000ì›',            # íŠ¹ì • ì¼€ì´ìŠ¤
			r'2\.000': '2,000',               # 2.000 -> 2,000
			r'(\d)\.(\d{3})': r'\1,\2',       # ìˆ«ì.000 -> ìˆ«ì,000
		}
		
		corrected_text = text
		for pattern, replacement in corrections.items():
			corrected_text = re.sub(pattern, replacement, corrected_text)
		
		return corrected_text

	def _merge_segment_results(self, segment_results):
		"""
		ë¶„í• ëœ ì´ë¯¸ì§€ë“¤ì˜ OCR ê²°ê³¼ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©
		ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±° (í…ìŠ¤íŠ¸ ê¸°ì¤€!)
		"""
		if len(segment_results) == 1:
			text_lines, y_offset = segment_results[0]
			return [(line, y_offset) for line in text_lines]

		all_text_lines = []
		seen_texts = set()  # ì´ë¯¸ ë“±ì¥í•œ í…ìŠ¤íŠ¸

		def normalize_text(text):
			# ê¸°í˜¸, ê³µë°±, HTML íƒœê·¸ ì œê±° + ì†Œë¬¸ì ë³€í™˜
			text = re.sub(r'<.*?>', '', text)
			text = re.sub(r'[\sÂ·â€¢\-â€“â€”,./\\(){}[\]|:;]', '', text)
			return text.strip().lower()

		for i, (text_lines, y_offset) in enumerate(segment_results):
			for line in text_lines:
				norm_text = normalize_text(line.text)
				# ì´ë¯¸ ë³¸ í…ìŠ¤íŠ¸(í˜¹ì€ í¬í•¨ëœ í…ìŠ¤íŠ¸)ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
				if norm_text in seen_texts or any(norm_text in t or t in norm_text for t in seen_texts):
					if self.debug:
						print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°(í…ìŠ¤íŠ¸ê¸°ë°˜): {line.text[:40]}")
					continue
				seen_texts.add(norm_text)
				all_text_lines.append((line, y_offset))
		
		if self.debug: 
			print(f"âœ… ë³‘í•© ì™„ë£Œ: ì´ {len(all_text_lines)}ê°œ í…ìŠ¤íŠ¸")
		return all_text_lines

	def _sort_by_position_with_segments(self, text_lines_with_offset):
		"""
		ì˜¤í”„ì…‹ ì •ë³´ë¥¼ ê³ ë ¤í•œ ì½ê¸° ìˆœì„œ ì •ë ¬ (ì ˆëŒ€ Y ê¸°ì¤€ ì „ì²´ ì •ë ¬)
		"""
		if not text_lines_with_offset:
			return text_lines_with_offset

		# ê° í…ìŠ¤íŠ¸ë¼ì¸ì˜ ì ˆëŒ€ y_centerì™€ x_center ê³„ì‚°
		abs_lines = []
		for line, y_offset in text_lines_with_offset:
			y_center = ((line.bbox[1] + line.bbox[3]) / 2) + y_offset
			x_center = (line.bbox[0] + line.bbox[2]) / 2
			abs_lines.append((line, y_offset, y_center, x_center))

		# ë¨¼ì € ì ˆëŒ€ y_centerë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
		abs_lines.sort(key=lambda x: (x[2], x[3]))

		# ê²°ê³¼ëŠ” (line, y_offset)ë§Œ ë°˜í™˜
		return [(line, y_offset) for line, y_offset, _, _ in abs_lines]

	def process_image_ocr(self, image_path):
		"""
		ê¸´ ì´ë¯¸ì§€ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•˜ì—¬ OCR ì²˜ë¦¬
		
		Args:
			image_path: ì´ë¯¸ì§€ ê²½ë¡œ
			
		Returns:
			OCR ê²°ê³¼ í…ìŠ¤íŠ¸ ë¼ì¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
		"""
		print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹œì‘")
		print("="*50)
		
		# Surya ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
		self._init_predictors()
		
		# 1. ì´ë¯¸ì§€ ë¶„í• 
		image_segments = self.smart_image_split(image_path)
		
		if len(image_segments) == 1:
			print("ğŸ“„ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬")
			
			# ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
			processed_img = self.enhanced_preprocess(image_path)
			predictions = self.recognition_predictor([processed_img], det_predictor=self.detection_predictor)
			
			# ë‹¨ì¼ ì´ë¯¸ì§€ì¸ ê²½ìš°ë„ (text_line, offset) í˜•íƒœë¡œ ë³€í™˜
			if predictions and predictions[0].text_lines:
				final_text_lines_with_offset = [(line, 0) for line in predictions[0].text_lines]
			else:
				final_text_lines_with_offset = []
		
		else:
			print(f"ğŸ“‘ ë‹¤ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ({len(image_segments)}ê°œ)")
			
			# 2. ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ OCR ì²˜ë¦¬
			segment_results = []
			
			for i, (segment_img, y_offset) in enumerate(image_segments):
				print(f"\nğŸ” ì„¸ê·¸ë¨¼íŠ¸ {i+1}/{len(image_segments)} ì²˜ë¦¬ ì¤‘...")
				
				# ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „ì²˜ë¦¬
				# ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì „ì²˜ë¦¬
				temp_path = f"temp_segment_{i}.png"
				segment_img.save(temp_path)
				
				try:
					processed_segment = self.enhanced_preprocess(temp_path)
					predictions = self.recognition_predictor([processed_segment], det_predictor=self.detection_predictor)
					
					if predictions and predictions[0].text_lines:
						segment_results.append((predictions[0].text_lines, y_offset))
						print(f"  âœ… {len(predictions[0].text_lines)}ê°œ í…ìŠ¤íŠ¸ ê²€ì¶œ")
					else:
						print("  âŒ í…ìŠ¤íŠ¸ ì—†ìŒ")
						
				except Exception as e:
					print(f"  âŒ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
				
				# ì„ì‹œ íŒŒì¼ ì •ë¦¬
				if os.path.exists(temp_path):
					os.remove(temp_path)
			
			# 3. ê²°ê³¼ ë³‘í•©
			print(f"\nğŸ”— ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼ ë³‘í•© ì¤‘...")
			final_text_lines_with_offset = self._merge_segment_results(segment_results)
		
		# 4. ìµœì¢… ì •ë ¬ ë° ì¶œë ¥
		sorted_text_lines_with_offset = self._sort_by_position_with_segments(final_text_lines_with_offset)
		
		print(f"\nğŸ“„ ìµœì¢… ê²°ê³¼ ({len(sorted_text_lines_with_offset)}ê°œ í…ìŠ¤íŠ¸)")
		print("-" * 70)
		
		# ì‹ ë¢°ë„ í†µê³„ ê³„ì‚°
		confidences = []
		final_text_lines = []
		
		for i, (text_line, y_offset) in enumerate(sorted_text_lines_with_offset, 1):
			original_text = text_line.text
			corrected_text = self.fix_korean_ocr_errors(original_text)
			
			# ì‹ ë¢°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
			confidence = getattr(text_line, 'confidence', None)
			if confidence is None:
				confidence = 1.0  # ì‹ ë¢°ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
			
			confidences.append(confidence)
			
			# ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ í‘œì‹œ (ì´ëª¨ì§€ë¡œ)
			confidence_icon = self._get_confidence_icon(confidence)
			confidence_str = f"{confidence:.3f}"
			
			# ìˆ˜ì • ì—¬ë¶€ í‘œì‹œ
			correction_mark = "ğŸ“" if original_text != corrected_text else "  "
			
			# ì¶œë ¥ í˜•ì‹: ë²ˆí˜¸. [ì‹ ë¢°ë„] í…ìŠ¤íŠ¸
			print(f"{i:3d}. {confidence_icon}[{confidence_str}] {correction_mark} {corrected_text}")
			
			# ìµœì¢… ê²°ê³¼ì—ëŠ” ì›ë³¸ text_line ê°ì²´ ì¶”ê°€
			final_text_lines.append(text_line)
		
		# ì‹ ë¢°ë„ í†µê³„ ì¶œë ¥
		if confidences:
			avg_confidence = sum(confidences) / len(confidences)
			min_confidence = min(confidences)
			max_confidence = max(confidences)
			low_confidence_count = sum(1 for c in confidences if c < 0.8)
			
			print(f"\nğŸ“Š ì‹ ë¢°ë„ í†µê³„")
			print(f"   í‰ê· : {avg_confidence:.3f} | ìµœì†Œ: {min_confidence:.3f} | ìµœëŒ€: {max_confidence:.3f}")
			print(f"   ë‚®ì€ ì‹ ë¢°ë„ (<0.8): {low_confidence_count}ê°œ ({low_confidence_count/len(confidences)*100:.1f}%)")
			
			# ì‹ ë¢°ë„ê°€ ë‚®ì€ í•­ëª©ë“¤ ë³„ë„ í‘œì‹œ
			if low_confidence_count > 0:
				print(f"\nâš ï¸ ê²€í† ê°€ í•„ìš”í•œ ë‚®ì€ ì‹ ë¢°ë„ í•­ëª©ë“¤:")
				low_conf_items = []
				for i, (text_line, y_offset) in enumerate(sorted_text_lines_with_offset, 1):
					confidence = getattr(text_line, 'confidence', 1.0)
					if confidence < 0.8:
						corrected_text = self.fix_korean_ocr_errors(text_line.text)
						low_conf_items.append((i, confidence, corrected_text))
				
				# ì‹ ë¢°ë„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
				low_conf_items.sort(key=lambda x: x[1])
				
				for line_num, conf, text in low_conf_items[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
					print(f"   {line_num:3d}ë²ˆ: [{conf:.3f}] {text[:50]}...")
				
				if len(low_conf_items) > 5:
					print(f"   ... ì™¸ {len(low_conf_items) - 5}ê°œ ë”")
		
		print(f"\nğŸ’¡ ì‹ ë¢°ë„ ë²”ë¡€: ğŸŸ¢ë†’ìŒ(>0.9) ğŸŸ¡ë³´í†µ(0.7-0.9) ğŸ”´ë‚®ìŒ(<0.7)")
		print(f"ğŸ’¡ ìˆ˜ì • í‘œì‹œ: ğŸ“ = OCR í›„ ìë™ ìˆ˜ì •ë¨")
		print(f"\n{'='*50}")
		print("ğŸ‰ ìŠ¤ë§ˆíŠ¸ OCR ì²˜ë¦¬ ì™„ë£Œ!")
		
		return final_text_lines

	def paddle_reocr_low_conf_lines(self, image_path, text_lines, conf_threshold=0.8, 
								   crop_margin=8, scale_factor=2.0, use_gpu=True):
		"""
		Surya ê²°ê³¼ ì¤‘ ì‹ ë¢°ë„ conf_threshold ì´í•˜ ì¤„ë§Œ í•´ë‹¹ bbox ì˜ì—­ cropâ†’ì „ì²˜ë¦¬â†’PaddleOCR í›„ ê²°ê³¼ ë¹„êµ.
		
		Args:
			image_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ
			text_lines: process_image_ocrì˜ ë°˜í™˜ê°’(list of text_line)
			conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
			crop_margin: í¬ë¡­ ì‹œ ì¶”ê°€ ì—¬ë°±
			scale_factor: ì´ë¯¸ì§€ í™•ëŒ€ ë°°ìœ¨
			use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
			
		Returns:
			ë¹„êµ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
		"""
		self._init_paddle_ocr(use_gpu)
		
		ori_img = cv2.imread(image_path)
		if ori_img is None:
			raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

		results = []
		for idx, line in enumerate(text_lines, 1):
			conf = getattr(line, "confidence", 1.0)
			if conf > conf_threshold:
				continue
				
			bbox = getattr(line, "bbox", None)
			if not bbox:  # bbox ì—†ëŠ” ì¤„ì€ ìŠ¤í‚µ
				continue
				
			# bbox ì •ìˆ˜í™”, margin ì ìš©
			x1, y1, x2, y2 = map(int, bbox)
			x1 = max(0, x1 - crop_margin)
			y1 = max(0, y1 - crop_margin)
			x2 = min(ori_img.shape[1], x2 + crop_margin)
			y2 = min(ori_img.shape[0], y2 + crop_margin)
			
			crop_img = ori_img[y1:y2, x1:x2]
			if crop_img is None or crop_img.size == 0 or crop_img.shape[0] == 0 or crop_img.shape[1] == 0:
				if self.debug:
					print(f"âš ï¸ [ê²½ê³ ] Crop ì‹¤íŒ¨ - ë¼ì¸ {idx}: bbox ì¢Œí‘œ(x1={x1},y1={y1},x2={x2},y2={y2})ê°€ ë¹„ì •ìƒ. ê±´ë„ˆëœ€.")
				continue
				
			temp_crop_path = f"temp_paddle_crop_{idx}.png"
			cv2.imwrite(temp_crop_path, crop_img)
			
			# ì „ì²˜ë¦¬(í™•ëŒ€, ì„ ëª…ë„ ë“±)
			img = Image.open(temp_crop_path)
			width, height = img.size
			img = img.resize((int(width*scale_factor), int(height*scale_factor)), Image.LANCZOS)
			img_pil = ImageEnhance.Sharpness(img).enhance(1.5)
			img_pil = ImageEnhance.Contrast(img_pil).enhance(1.2)
			img_pil.save(temp_crop_path)
			
			# PaddleOCR ì‹¤í–‰
			paddle_result = self.paddle_ocr.ocr(temp_crop_path, cls=True)
			
			# ê²°ê³¼ ì •ë¦¬
			paddle_words = []
			for line_result in paddle_result:
				if line_result:
					for word_info in line_result:
						word, paddle_conf = word_info[1][0], word_info[1][1]
						paddle_words.append((word, paddle_conf))
			
			best_word, best_conf = ("", 0.0)
			if paddle_words:
				best_word, best_conf = max(paddle_words, key=lambda x: x[1])
			
			results.append({
				"idx": idx,
				"surya_text": line.text,
				"surya_conf": conf,
				"paddle_text": best_word,
				"paddle_conf": best_conf,
			})
			
			if self.debug:
				print(f"\n[{idx}] Surya({conf:.3f}): {line.text}")
				print(f"    Paddle({best_conf:.3f}): {best_word}")
			
			# ì„ì‹œ íŒŒì¼ ì‚­ì œ
			if os.path.exists(temp_crop_path):
				os.remove(temp_crop_path)
		
		return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
	# OCR í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
	ocr_processor = OCRProcessor(max_height=2000, overlap_ratio=0.1, debug=True)
	
	# ì´ë¯¸ì§€ ê²½ë¡œ
	image_path = "/Users/parkjunseo/Downloads/ë°•ì¤€ì„œ_ì¡¸ì—…ì¦ëª…ì„œ.jpg"  # ì—¬ê¸°ì— ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥

	# ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
	if not os.path.exists(image_path):
		print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
		print("í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì‚¬ìš©í•˜ë ¤ë©´ image_pathë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
		import sys
		sys.exit(1)
	
	# OCR ì²˜ë¦¬
	result = ocr_processor.process_image_ocr(image_path)
	
	# ê²°ê³¼ ì¶œë ¥
	print("\nìµœì¢… OCR ê²°ê³¼:")
	for idx, text_line in enumerate(result, 1):
		print(f"{idx}. {text_line.text} (ì‹ ë¢°ë„: {getattr(text_line, 'confidence', 1.0):.3f})")
	